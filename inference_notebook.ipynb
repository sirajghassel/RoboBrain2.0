{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/robobrain2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/directory')\n",
    "from inference import UnifiedInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/robobrain2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from inference import UnifiedInference\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(1, device=\"mps\")\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model thinking support: True\n",
      "\n",
      "==================== INPUT ====================\n",
      "What is shown in this image?\n",
      "===============================================\n",
      "\n",
      "Thinking disabled (but supported).\n",
      "Running inference ...\n",
      "Prediction:\n",
      "{'answer': 'Two tabby cats are shown sleeping next to each other on a couch or pink blanket.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPrediction: (as an example)\\n{\\n        'thinking': '', \\n        'answer': 'Two cats sleeping side by side on a couch.'\\n}\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UnifiedInference(\"BAAI/RoboBrain2.0-7B\")\n",
    "\n",
    "prompt = \"What is shown in this image?\"\n",
    "image = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "pred = model.inference(prompt, image, task=\"general\", enable_thinking=False, do_sample=True)\n",
    "print(f\"Prediction:\\n{pred}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction: (as an example)\n",
    "{\n",
    "        'thinking': '', \n",
    "        'answer': 'Two cats sleeping side by side on a couch.'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== INPUT ====================\n",
      "What is shown in this image?\n",
      "===============================================\n",
      "\n",
      "Thinking enabled.\n",
      "Running inference ...\n",
      "Prediction:\n",
      "{'answer': 'The image shows two cats lying next to each other on top of a couch.', 'thinking': 'Within the visual input, there are two cats lying next to each other on a couch. They appear to be resting comfortably and sleeping as they occupy a large section of the couch cushion. This cozy scene gives off a sense of warmth and tranquility.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPrediction: (as an example)\\n{\\n        'thinking': 'Upon examining the visual input, I observe two cats resting comfortably on a pink blanket that covers a couch or sofa. The cats are lying side by side, with one on top of the other, indicating their relaxed state and possibly their close bond. Their positions suggest they feel safe and at ease in their environment.\\n\\nWith my advanced visual processing capabilities, I can identify various objects within this scene, such as the pink blanket beneath the cats and the couch they are lying on. Additionally, there appear to be remote controls nearby, potentially situated on or near the couch, which further confirms that this is an indoor setting where people relax and spend leisure time.', \\n        'answer': 'The image shows two cats lying on a pink blanket on a couch.'\\n}\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Predict with thinking (General)\n",
    "\n",
    "# from inference import UnifiedInference\n",
    "# model = UnifiedInference(\"BAAI/RoboBrain2.0-7B\")\n",
    "\n",
    "prompt = \"What is shown in this image?\"\n",
    "image = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "pred = model.inference(prompt, image, task=\"general\", enable_thinking=True, do_sample=True)\n",
    "print(f\"Prediction:\\n{pred}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction: (as an example)\n",
    "{\n",
    "        'thinking': 'Upon examining the visual input, I observe two cats resting comfortably on a pink blanket that covers a couch or sofa. The cats are lying side by side, with one on top of the other, indicating their relaxed state and possibly their close bond. Their positions suggest they feel safe and at ease in their environment.\\n\\nWith my advanced visual processing capabilities, I can identify various objects within this scene, such as the pink blanket beneath the cats and the couch they are lying on. Additionally, there appear to be remote controls nearby, potentially situated on or near the couch, which further confirms that this is an indoor setting where people relax and spend leisure time.', \n",
    "        'answer': 'The image shows two cats lying on a pink blanket on a couch.'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grounding task detected. Adding grounding prompt.\n",
      "\n",
      "==================== INPUT ====================\n",
      "Please provide the bounding box coordinate of the region this sentence describes: the person wearing a red hat.\n",
      "===============================================\n",
      "\n",
      "Thinking enabled.\n",
      "Running inference ...\n",
      "Plotting enabled. Drawing results on the image ...\n",
      "Extracted bounding boxes: []\n",
      "Annotated image saved to: result/grounding_with_grounding_annotated.jpg\n",
      "Prediction:\n",
      "{'answer': '[0, 192, 270, 6', 'thinking': \"From the visual input, there is a child in a red cap who stands out out prominently due to the vivid color and the position within the image. This child is seated next to an adult, both engaged in an activity involving a banana peel. The child's facial expression suggests excitement or amusement, which is evident from the open mouth and laughter.\\n\\nThe task requires identifying the bounding box around the person wearing the red cap. To achieve this, I begin by focusing on the physical characteristics that define the child, such as the red cap and bright blue outfit with stripes. Combining these elements with the spatial context—specifically the child's location relative to other objects in the scene—enables the identification of his area of presence in the frame.\\n\\nThe child's head and upper torso occupy a significant portion of the left side of the image, while the arm extends towards the adult, adding additional spatial markers for accurate bounding. Verification confirms that this spatial configuration aligns with the visual features linked to the child wearing the red cap.\\n\\nBased on my direct observation and analysis, the red cap-wearing child is confined within the specific boundaries delineated as [1 [5, 1\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrediction: (as an example)\\n{\\n    \\'thinking\\': \"From the visual input, I can identify two individuals: a man and a young boy. The man appears to be seated outside against a stone wall, wearing a blue top and jeans. His hands are around the young boy\\'s waist. The boy is wearing a red baseball cap and a striped sweater, and he seems to be laughing or having fun while interacting with the man.\\n\\nNow focusing on the task at hand, which involves identifying the person wearing a red hat. In this scenario, it would be reasonable to deduce that the boy, who is wearing a red baseball cap, is the one wearing the red hat. The red cap stands out against the other elements in the scene due to its bright color, making it easy to pinpoint as the object in question.\\n\\nTherefore, based on direct visual analysis, the person wearing the red hat is indeed the young boy, and his position relative to the man is such that he is seated close to him along the stone wall.\", \\n    \\'answer\\': \\'[0, 193, 226, 640]\\'\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Usage for Visual Grounding (VG)\n",
    "\n",
    "# from inference import UnifiedInference\n",
    "# model = UnifiedInference(\"BAAI/RoboBrain2.0-7B\")\n",
    "\n",
    "prompt = \"the person wearing a red hat\"\n",
    "image = \"./assets/demo/grounding.jpg\"\n",
    "\n",
    "pred = model.inference(prompt, image, task=\"grounding\", plot=True, enable_thinking=True, do_sample=True)\n",
    "print(f\"Prediction:\\n{pred}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction: (as an example)\n",
    "{\n",
    "    'thinking': \"From the visual input, I can identify two individuals: a man and a young boy. The man appears to be seated outside against a stone wall, wearing a blue top and jeans. His hands are around the young boy's waist. The boy is wearing a red baseball cap and a striped sweater, and he seems to be laughing or having fun while interacting with the man.\\n\\nNow focusing on the task at hand, which involves identifying the person wearing a red hat. In this scenario, it would be reasonable to deduce that the boy, who is wearing a red baseball cap, is the one wearing the red hat. The red cap stands out against the other elements in the scene due to its bright color, making it easy to pinpoint as the object in question.\\n\\nTherefore, based on direct visual analysis, the person wearing the red hat is indeed the young boy, and his position relative to the man is such that he is seated close to him along the stone wall.\", \n",
    "    'answer': '[0, 193, 226, 640]'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affordance task detected. Adding affordance prompt.\n",
      "\n",
      "==================== INPUT ====================\n",
      "You are a robot using the joint control. The task is \"hold the cup\". Please predict a possible affordance area of the end effector.\n",
      "===============================================\n",
      "\n",
      "Thinking enabled.\n",
      "Running inference ...\n",
      "Plotting enabled. Drawing results on the image ...\n",
      "Extracted bounding boxes: []\n",
      "Annotated image saved to: result/affordance_with_affordance_annotated.jpg\n",
      "Prediction:\n",
      "{'answer': '[566, 204, 6', 'thinking': \"From the visual input, the object is recognized as a white mug filled with a dark liquid, likely coffee. It has a cylindrical body with a small round handle on one side. Its height appears to be around 10 cm and its is sitting stably on a wooden surface. The material seems smooth and ceramic-like, deduced from the reflection on the surface. \\n\\nMy gripper can accommodate objects with a small profile such as this handle, which visually seems to be within the typical grip span of my end-effector. My joint control allows me to align precisely for grasping tasks, ensuring stability during lifting or moving.\\n\\nThe current context requires holding the cup, suggesting the need to grasp it securely without spilling the contents. The handle provides an ideal anchor point for a firm hold due to its its its its well-defined edges that ensure stability during manipulation. \\n\\nUpon verifying, I confirm that my reach and reachability allow direct contact with the handle, given the mug's position on the table. \\n\\nTherefore, the cup's affordance area is identified at [564, 204, 6\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrediction: (as an example)\\n{\\n    \\'thinking\\': \"From the visual input, the object is recognized as a white ceramic cup with a handle on its side. It appears cylindrical with an open top and has sufficient height for a standard drinking cup. The handle is positioned to one side, which is crucial for grasping. The cup rests on a wooden surface, suggesting stability due to its weight and material solidity.\\n\\nMy end-effector is equipped with a gripper capable of securely engaging objects of this size and shape, specifically designed for cylindrical and handle-like surfaces. Given my capabilities, I can adjust the grip to accommodate the handle\\'s size and curve. The gripper can easily access the handle area without disturbing the cup\\'s balance on the flat surface.\\n\\nThe current task is to hold the cup, which necessitates securely gripping it by the handle or potentially enveloping the body if necessary. The cup’s position on the table, within reach, allows me to approach from the left side toward the handle, ensuring optimal leverage for lifting. \\n\\nVerifying the handle\\'s suitability, it seems sufficiently robust and unobstructed to enable a reliable grip. My sensors will ensure that the force applied through the gripper doesn\\'t exceed the cup\\'s weight and stability limits.\\n\\nTherefore, the cup\\'s affordance area is [577, 224, 638, 310]. This is because the handle provides a clear and accessible spot for my gripper to engage securely, fulfilling the task requirement to hold the cup effectively.\", \\n    \\'answer\\': \\'[577, 224, 638, 310]\\'\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 4. Usage for Affordance Prediction (Embodied)\n",
    "\n",
    "# from inference import UnifiedInference\n",
    "# model = UnifiedInference(\"BAAI/RoboBrain2.0-7B\")\n",
    "\n",
    "# Example:\n",
    "prompt = \"hold the cup\"\n",
    "image = \"./assets/demo/affordance.jpg\"\n",
    "\n",
    "pred = model.inference(prompt, image, task=\"affordance\", plot=True, enable_thinking=True, do_sample=True)\n",
    "print(f\"Prediction:\\n{pred}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction: (as an example)\n",
    "{\n",
    "    'thinking': \"From the visual input, the object is recognized as a white ceramic cup with a handle on its side. It appears cylindrical with an open top and has sufficient height for a standard drinking cup. The handle is positioned to one side, which is crucial for grasping. The cup rests on a wooden surface, suggesting stability due to its weight and material solidity.\\n\\nMy end-effector is equipped with a gripper capable of securely engaging objects of this size and shape, specifically designed for cylindrical and handle-like surfaces. Given my capabilities, I can adjust the grip to accommodate the handle's size and curve. The gripper can easily access the handle area without disturbing the cup's balance on the flat surface.\\n\\nThe current task is to hold the cup, which necessitates securely gripping it by the handle or potentially enveloping the body if necessary. The cup’s position on the table, within reach, allows me to approach from the left side toward the handle, ensuring optimal leverage for lifting. \\n\\nVerifying the handle's suitability, it seems sufficiently robust and unobstructed to enable a reliable grip. My sensors will ensure that the force applied through the gripper doesn't exceed the cup's weight and stability limits.\\n\\nTherefore, the cup's affordance area is [577, 224, 638, 310]. This is because the handle provides a clear and accessible spot for my gripper to engage securely, fulfilling the task requirement to hold the cup effectively.\", \n",
    "    'answer': '[577, 224, 638, 310]'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory task detected. Adding trajectory prompt.\n",
      "\n",
      "==================== INPUT ====================\n",
      "You are a robot using the joint control. The task is \"reach for the banana on the plate\". Please predict up to 10 key trajectory points to complete the task. Your answer should be formatted as a list of tuples, i.e. [[x1, y1], [x2, y2], ...], where each tuple contains the x and y coordinates of a point.\n",
      "===============================================\n",
      "\n",
      "Thinking enabled.\n",
      "Running inference ...\n",
      "Plotting enabled. Drawing results on the image ...\n",
      "Extracted trajectory points: [[(117, 118), (153, 96), (178, 87), (220, 81)]]\n",
      "Annotated image saved to: result/trajectory_with_trajectory_annotated.jpg\n",
      "Prediction:\n",
      "{'answer': '[(117, 118), (153, 96), (178, 87), (220, 81)]', 'thinking': \"From the visual input, the target object, a banana, is clearly identified resting upon a plate near the top center area. My end-effector is positioned at the left side of the workspace. In terms of spatial relationships, the banana's location seems slightly elevated compared to my current end-effector position, indicating a need for an upward movement initially. No immediate obstacles are evident between my current position and the banana, although there are several smaller objects, such as bowls and plates, that could potentially obstruct a direct route if not accounted for.\\n\\nMy joint control system allows me to plan an efficient path by generating smooth, controlled movements. I will begin by elevating my end-effector to clear any potential obstructions. The trajectory will involve a sequence of upward and forward movements followed by lateral adjustments towards the banana on the plate. This strategy ensures that the path remains clear of any nearby items while maintaining a precise approach.\\n\\nThe task requires reaching for the banana on the plate, suggesting the need for a direct-to target motion. Given the simplicity of the scene and the absence of major obstructions, this entails a short but carefully planned path, requiring fewer than ten key trajectory points.\\n\\nAfter the planning process, I will mentally verify the sequence, ensuring no collisions will occur with any objects in close proximity. Each segment of the trajectory is checked to ensure adequate clearance, especially from the nearby bowl structures. The final point must precisely target the banana on the plate.\\n\\nTherefore, based on direct visual analysis, my motion planning capabilities, and task requirements, the key trajectory points to reach the banana are determined as [[117, 118], [153, 96], [178, 87], [220, 81]]. These points form a viable path from the initial end-effector position to the target, adhering to the visual environment.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrediction: (as an example)\\n{\\n    \\'thinking\\': \\'From the visual input, the target object, a banana, is placed upon a circular plate towards the center-right of the table. My end-effector is positioned near the left edge of the table, ready to initiate movement. A spatial analysis of the scene reveals potential obstructions such as various dishes and objects around the plate. The plate itself defines the immediate vicinity of the target.\\n\\nMy joint control system enables me to generate smooth trajectories for my end-effector. I will plan a sequence starting from my current position, moving across the table until it reaches the banana, while ensuring clearance from obstacles. The trajectory must be efficient in reaching the target without unnecessary detours or collisions.\\n\\nThe task is to \"reach for the banana on the plate\", necessitating a path that begins at my current location and terminates at or very near the banana. Up to 10 key points can be utilized, but fewer may suffice if the path is straightforward.\\n\\nI verify the proposed path by mentally simulating the trajectory. Considering the table layout, the path needs to navigate away from the nearest glass and avoid the bottle on the right-hand side. Each segment of the trajectory should present sufficient clearance from these objects. The final point must precisely align with the banana\\'s location on the plate.\\n\\nTherefore, based on direct vision analysis, motion planning capabilities, and task requirements, the trajectory points to reach the banana are determined as follows: [(137, 116), (169, 94), (208, 84), (228, 80)]. This sequence forms a viable path from the current end-effector position to the target, respecting the visual environment.\\', \\n    \\'answer\\': \\'[(137, 116), (169, 94), (208, 84), (228, 80)]\\'\\n}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 5. Usage for Trajectory Prediction (Embodied)\n",
    "\n",
    "# from inference import UnifiedInference\n",
    "# model = UnifiedInference(\"BAAI/RoboBrain2.0-7B\")\n",
    "\n",
    "# Example:\n",
    "prompt = \"reach for the banana on the plate\"\n",
    "\n",
    "image = \"./assets/demo/trajectory.jpg\"\n",
    "\n",
    "pred = model.inference(prompt, image, task=\"trajectory\", plot=True, enable_thinking=True, do_sample=True)\n",
    "print(f\"Prediction:\\n{pred}\")\n",
    "\n",
    "\"\"\"\n",
    "Prediction: (as an example)\n",
    "{\n",
    "    'thinking': 'From the visual input, the target object, a banana, is placed upon a circular plate towards the center-right of the table. My end-effector is positioned near the left edge of the table, ready to initiate movement. A spatial analysis of the scene reveals potential obstructions such as various dishes and objects around the plate. The plate itself defines the immediate vicinity of the target.\\n\\nMy joint control system enables me to generate smooth trajectories for my end-effector. I will plan a sequence starting from my current position, moving across the table until it reaches the banana, while ensuring clearance from obstacles. The trajectory must be efficient in reaching the target without unnecessary detours or collisions.\\n\\nThe task is to \"reach for the banana on the plate\", necessitating a path that begins at my current location and terminates at or very near the banana. Up to 10 key points can be utilized, but fewer may suffice if the path is straightforward.\\n\\nI verify the proposed path by mentally simulating the trajectory. Considering the table layout, the path needs to navigate away from the nearest glass and avoid the bottle on the right-hand side. Each segment of the trajectory should present sufficient clearance from these objects. The final point must precisely align with the banana\\'s location on the plate.\\n\\nTherefore, based on direct vision analysis, motion planning capabilities, and task requirements, the trajectory points to reach the banana are determined as follows: [(137, 116), (169, 94), (208, 84), (228, 80)]. This sequence forms a viable path from the current end-effector position to the target, respecting the visual environment.', \n",
    "    'answer': '[(137, 116), (169, 94), (208, 84), (228, 80)]'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robobrain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
